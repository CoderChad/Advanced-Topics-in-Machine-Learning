{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMJ3AmgK4iLo"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "DS-UA 301 Advanced Topics in Data Science\n",
        "Homework 3 Solution\n",
        "\n",
        "This file implements the solutions for Homework 3, focusing on:\n",
        "- Problem 1: Learning Rate, Batch Size, FashionMNIST\n",
        "- Problem 3: Transfer Learning\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.datasets import FashionMNIST\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import copy\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#                  Problem 1: Learning Rate, Batch Size, FashionMNIST      #"
      ],
      "metadata": {
        "id": "5tcrd-XqSFxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        # First convolutional layer\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2)\n",
        "        # Second convolutional layer\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First conv block\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # Second conv block\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def load_fashion_mnist(batch_size=64):\n",
        "    \"\"\"\n",
        "    Load the FashionMNIST dataset\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    # Load training data\n",
        "    train_set = FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "    # Split into train and validation sets\n",
        "    train_size = int(0.8 * len(train_set))\n",
        "    val_size = len(train_set) - train_size\n",
        "    train_dataset, val_dataset = random_split(train_set, [train_size, val_size])\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Load test data\n",
        "    test_set = FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "class CyclicLR:\n",
        "    \"\"\"\n",
        "    Cyclical Learning Rate scheduler with exponential decay\n",
        "    Based on the paper: \"Cyclical Learning Rates for Training Neural Networks\"\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, base_lr, max_lr, step_size, gamma=1.0):\n",
        "        self.optimizer = optimizer\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.gamma = gamma\n",
        "        self.clr_iterations = 0\n",
        "        self.cycle_momentum = True\n",
        "\n",
        "        if self.cycle_momentum:\n",
        "            for group in self.optimizer.param_groups:\n",
        "                group.setdefault('initial_momentum', group.get('momentum', 0))\n",
        "\n",
        "    def reset(self):\n",
        "        self.clr_iterations = 0\n",
        "\n",
        "    def step(self):\n",
        "        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
        "        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
        "\n",
        "        # Apply exponential decay\n",
        "        decay_factor = self.gamma ** self.clr_iterations\n",
        "\n",
        "        # Calculate the learning rate\n",
        "        lr = self.base_lr + (self.max_lr - self.base_lr) * max(0, (1 - x)) * decay_factor\n",
        "\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "            # Update momentum if cycle_momentum is True\n",
        "            if self.cycle_momentum:\n",
        "                if self.clr_iterations % (2 * self.step_size) < self.step_size:\n",
        "                    momentum = param_group['initial_momentum'] + (0.99 - param_group['initial_momentum']) * (1 - x)\n",
        "                else:\n",
        "                    momentum = param_group['initial_momentum']\n",
        "                param_group['momentum'] = momentum\n",
        "\n",
        "        self.clr_iterations += 1\n",
        "        return lr\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, epochs=5, device=device):\n",
        "    \"\"\"\n",
        "    Train the model and return training history\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': [],\n",
        "        'lr': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track the current learning rate\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "            # Update learning rate if using scheduler\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            # Print progress\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch: {epoch+1}/{epochs} | Batch: {batch_idx}/{len(train_loader)} | '\n",
        "                      f'Loss: {running_loss/(batch_idx+1):.3f} | '\n",
        "                      f'Acc: {100.*correct/total:.3f}% | '\n",
        "                      f'LR: {current_lr:.6f}')\n",
        "\n",
        "        # Compute epoch-level statistics for training\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100. * correct / total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                # Update statistics\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        # Compute epoch-level statistics for validation\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_acc = 100. * correct / total\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['lr'].append(current_lr)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f'Epoch: {epoch+1}/{epochs} | '\n",
        "              f'Train Loss: {train_loss:.3f} | '\n",
        "              f'Train Acc: {train_acc:.3f}% | '\n",
        "              f'Val Loss: {val_loss:.3f} | '\n",
        "              f'Val Acc: {val_acc:.3f}% | '\n",
        "              f'LR: {current_lr:.6f}')\n",
        "\n",
        "    return history\n",
        "\n",
        "def find_lr(model_class, train_loader, val_loader, epochs=5, start_lr=1e-9, end_lr=10, device=device):\n",
        "    \"\"\"\n",
        "    Run training for different learning rates and find the optimal range\n",
        "    \"\"\"\n",
        "    # Initialize arrays to store results\n",
        "    learning_rates = np.logspace(np.log10(start_lr), np.log10(end_lr), num=10)\n",
        "    final_losses = []\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        print(f\"\\nTraining with learning rate: {lr}\")\n",
        "\n",
        "        # Initialize model and optimizer\n",
        "        model = model_class().to(device)\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Train for specified epochs\n",
        "        history = train_model(model, train_loader, val_loader, criterion, optimizer, epochs=epochs)\n",
        "\n",
        "        # Store the final training loss\n",
        "        final_losses.append(history['train_loss'][-1])\n",
        "\n",
        "    # Convert to numpy arrays for plotting\n",
        "    log_lrs = np.log10(learning_rates)\n",
        "\n",
        "    # Plot Loss vs Learning Rate\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(log_lrs, final_losses, 'o-', markersize=8)\n",
        "    plt.xlabel('Log Learning Rate')\n",
        "    plt.ylabel('Final Training Loss')\n",
        "    plt.title('Training Loss vs. Learning Rate')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('lr_finder.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Determine optimal lr_min and lr_max\n",
        "    # Typically, lr_min is where the loss starts to decrease rapidly\n",
        "    # lr_max is just before the loss starts to diverge\n",
        "    best_loss_idx = np.argmin(final_losses)\n",
        "    divergence_idx = np.argmax(final_losses)\n",
        "\n",
        "    # Compute indices for lr_min and lr_max\n",
        "    lr_min_idx = max(0, best_loss_idx - 1)  # One step before optimal\n",
        "    lr_max_idx = best_loss_idx  # The optimal point\n",
        "\n",
        "    lr_min = learning_rates[lr_min_idx]\n",
        "    lr_max = learning_rates[lr_max_idx]\n",
        "\n",
        "    print(f\"Suggested lr_min: {lr_min}\")\n",
        "    print(f\"Suggested lr_max: {lr_max}\")\n",
        "\n",
        "    return lr_min, lr_max, learning_rates, final_losses\n",
        "\n",
        "def train_with_clr(model_class, train_loader, val_loader, lr_min, lr_max, step_size=500, epochs=20, device=device):\n",
        "    \"\"\"\n",
        "    Train model with cyclical learning rate policy\n",
        "    \"\"\"\n",
        "    # Initialize model and optimizer\n",
        "    model = model_class().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr_min, momentum=0.9)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Initialize the CLR scheduler\n",
        "    clr = CyclicLR(optimizer, base_lr=lr_min, max_lr=lr_max, step_size=step_size, gamma=0.99)\n",
        "\n",
        "    # Train with CLR\n",
        "    history = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=clr, epochs=epochs)\n",
        "\n",
        "    # Plot training and validation metrics\n",
        "    epochs_range = range(1, epochs + 1)\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(epochs_range, history['train_loss'], 'o-', label='Training Loss')\n",
        "    plt.plot(epochs_range, history['val_loss'], 'o-', label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(epochs_range, history['train_acc'], 'o-', label='Training Accuracy')\n",
        "    plt.plot(epochs_range, history['val_acc'], 'o-', label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot learning rate\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(history['lr'], 'o-')\n",
        "    plt.title('Learning Rate over Training Steps')\n",
        "    plt.xlabel('Training Steps')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('clr_training.png')\n",
        "    plt.show()\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def batch_size_experiment(model_class, train_loader, val_loader, fixed_lr, device=device):\n",
        "    \"\"\"\n",
        "    Test the effect of increasing batch size with fixed learning rate\n",
        "    \"\"\"\n",
        "    batch_sizes = [32, 64, 128, 256, 512, 1024, 2048, 4096]\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "        print(f\"\\nTraining with batch size: {batch_size}\")\n",
        "\n",
        "        # Reload data with the new batch size\n",
        "        train_loader_bs, val_loader_bs, _ = load_fashion_mnist(batch_size=batch_size)\n",
        "\n",
        "        # Initialize model and optimizer\n",
        "        model = model_class().to(device)\n",
        "        optimizer = optim.SGD(model.parameters(), lr=fixed_lr, momentum=0.9)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Train for a few epochs\n",
        "        history = train_model(model, train_loader_bs, val_loader_bs, criterion, optimizer, epochs=5)\n",
        "\n",
        "        # Store the final training loss and validation accuracy\n",
        "        losses.append(history['train_loss'][-1])\n",
        "        accuracies.append(history['val_acc'][-1])\n",
        "\n",
        "    # Plot Loss vs Batch Size (log scale)\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(np.log2(batch_sizes), losses, 'o-', markersize=8)\n",
        "    plt.xlabel('log2(Batch Size)')\n",
        "    plt.ylabel('Final Training Loss')\n",
        "    plt.title('Training Loss vs. Batch Size')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(np.log2(batch_sizes), accuracies, 'o-', markersize=8)\n",
        "    plt.xlabel('log2(Batch Size)')\n",
        "    plt.ylabel('Validation Accuracy (%)')\n",
        "    plt.title('Validation Accuracy vs. Batch Size')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('batch_size_experiment.png')\n",
        "    plt.show()\n",
        "\n",
        "    return batch_sizes, losses, accuracies\n",
        "\n",
        "def run_problem1():\n",
        "    \"\"\"\n",
        "    Run Problem 1 experiments\n",
        "    \"\"\"\n",
        "    print(\"\\n\\n\" + \"=\"*80)\n",
        "    print(\"Problem 1: Learning Rate, Batch Size, FashionMNIST\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Load data\n",
        "    train_loader, val_loader, test_loader = load_fashion_mnist(batch_size=64)\n",
        "\n",
        "    # Part 1: Find optimal learning rate range\n",
        "    print(\"\\nPart 1: Finding optimal learning rate range...\")\n",
        "    lr_min, lr_max, learning_rates, final_losses = find_lr(LeNet5, train_loader, val_loader, epochs=5)\n",
        "\n",
        "    # Part 2: Train with cyclical learning rate\n",
        "    print(\"\\nPart 2: Training with cyclical learning rate...\")\n",
        "    model_clr, history_clr = train_with_clr(LeNet5, train_loader, val_loader, lr_min, lr_max, epochs=15)\n",
        "\n",
        "    # Part 3: Batch size experiment\n",
        "    print(\"\\nPart 3: Batch size experiment...\")\n",
        "    batch_sizes, losses, accuracies = batch_size_experiment(LeNet5, train_loader, val_loader, fixed_lr=lr_max)\n",
        "\n",
        "    print(\"\\nProblem 1 experiments completed.\")"
      ],
      "metadata": {
        "id": "C6RtQ1DMSC3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#                   Problem 2: CNN Architectures Analysis                    #"
      ],
      "metadata": {
        "id": "8CH4NZShSjiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "Problem 2 requires theoretical calculations and analysis. See the separate\n",
        "document for the detailed solutions for:\n",
        "\n",
        "1. Number of parameters in AlexNet\n",
        "2. VGG19 memory and parameters calculation\n",
        "3. Receptive field calculation\n",
        "4. Inception module analysis\n",
        "5. Faster-RCNN architecture analysis\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "XTjLW-vC4kWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3 - Transfer learning: Shallow learning vs Finetuning, Pytorch"
      ],
      "metadata": {
        "id": "BKgYCNhaSlVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_visual_decathlon_dataset():\n",
        "    \"\"\"\n",
        "    Download and prepare a dataset from the Visual Decathlon Challenge\n",
        "    For this implementation, we'll use CIFAR-100 as our target dataset\n",
        "    \"\"\"\n",
        "    # Define transformations\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "    ])\n",
        "\n",
        "    # Download CIFAR-100 dataset\n",
        "    train_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data', train=True, download=True, transform=train_transform\n",
        "    )\n",
        "\n",
        "    test_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data', train=False, download=True, transform=test_transform\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Analyze dataset\n",
        "    class_counts = {}\n",
        "    for _, label in train_dataset:\n",
        "        if label not in class_counts:\n",
        "            class_counts[label] = 0\n",
        "        class_counts[label] += 1\n",
        "\n",
        "    print(f\"Dataset: CIFAR-100\")\n",
        "    print(f\"Number of classes: {len(class_counts)}\")\n",
        "    print(f\"Total training samples: {len(train_dataset)}\")\n",
        "    print(f\"Total test samples: {len(test_dataset)}\")\n",
        "    print(f\"Average samples per class: {len(train_dataset) / len(class_counts):.2f}\")\n",
        "    print(f\"Min samples per class: {min(class_counts.values())}\")\n",
        "    print(f\"Max samples per class: {max(class_counts.values())}\")\n",
        "\n",
        "    return train_loader, test_loader, len(class_counts)\n",
        "\n",
        "def show_sample_images(data_loader, num_classes=2, images_per_class=2):\n",
        "    \"\"\"\n",
        "    Display sample images from the dataset\n",
        "    \"\"\"\n",
        "    # Get some random training images\n",
        "    dataiter = iter(data_loader)\n",
        "    images, labels = next(dataiter)\n",
        "\n",
        "    # Create a dictionary to store images by class\n",
        "    class_images = {}\n",
        "\n",
        "    # Keep fetching batches until we have enough images for each class\n",
        "    while len(class_images) < num_classes or any(len(imgs) < images_per_class for imgs in class_images.values()):\n",
        "        for img, lbl in zip(images, labels):\n",
        "            if lbl.item() not in class_images:\n",
        "                if len(class_images) >= num_classes:\n",
        "                    continue\n",
        "                class_images[lbl.item()] = []\n",
        "\n",
        "            if len(class_images[lbl.item()]) < images_per_class:\n",
        "                class_images[lbl.item()].append(img)\n",
        "\n",
        "        # Check if we have enough images\n",
        "        if len(class_images) >= num_classes and all(len(imgs) >= images_per_class for imgs in class_images.values()):\n",
        "            break\n",
        "\n",
        "        # Fetch more images if needed\n",
        "        try:\n",
        "            images, labels = next(dataiter)\n",
        "        except StopIteration:\n",
        "            break\n",
        "\n",
        "    # Plot images\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    classes = list(class_images.keys())\n",
        "\n",
        "    for i, class_idx in enumerate(classes[:num_classes]):\n",
        "        for j, img in enumerate(class_images[class_idx][:images_per_class]):\n",
        "            plt.subplot(num_classes, images_per_class, i * images_per_class + j + 1)\n",
        "            img = img.numpy().transpose((1, 2, 0))\n",
        "            # Denormalize\n",
        "            mean = np.array([0.5071, 0.4867, 0.4408])\n",
        "            std = np.array([0.2675, 0.2565, 0.2761])\n",
        "            img = std * img + mean\n",
        "            img = np.clip(img, 0, 1)\n",
        "            plt.imshow(img)\n",
        "            plt.title(f\"Class {class_idx}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('sample_images.png')\n",
        "    plt.show()\n",
        "\n",
        "def finetune_model(model, train_loader, test_loader, learning_rate, num_epochs=60, momentum=0.9, weight_decay=5e-4):\n",
        "    \"\"\"\n",
        "    Finetune a pretrained model on the target dataset\n",
        "    \"\"\"\n",
        "    # Define criterion and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "    # Define learning rate scheduler with multi-step decay\n",
        "    milestones = [int(num_epochs * 0.25), int(num_epochs * 0.5), int(num_epochs * 0.75)]\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n",
        "\n",
        "    # Initialize best accuracy and model\n",
        "    best_acc = 0.0\n",
        "    best_model = None\n",
        "\n",
        "    # Training and validation history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'test_loss': [],\n",
        "        'test_acc': [],\n",
        "        'lr': []\n",
        "    }\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            # Print progress\n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f'Epoch: {epoch+1}/{num_epochs} | Batch: {batch_idx}/{len(train_loader)} | '\n",
        "                      f'Loss: {running_loss/(batch_idx+1):.3f} | '\n",
        "                      f'Acc: {100.*correct/total:.3f}% | '\n",
        "                      f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "        # Compute epoch-level statistics for training\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100. * correct / total\n",
        "\n",
        "        # Testing phase\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in test_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                # Update statistics\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        # Compute epoch-level statistics for testing\n",
        "        test_loss = test_loss / len(test_loader)\n",
        "        test_acc = 100. * correct / total\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_acc'].append(test_acc)\n",
        "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f'Epoch: {epoch+1}/{num_epochs} | '\n",
        "              f'Train Loss: {train_loss:.3f} | '\n",
        "              f'Train Acc: {train_acc:.3f}% | '\n",
        "              f'Test Loss: {test_loss:.3f} | '\n",
        "              f'Test Acc: {test_acc:.3f}% | '\n",
        "              f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "        # Save best model\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            best_model = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(best_model)\n",
        "\n",
        "    # Plot training and validation metrics\n",
        "    epochs_range = range(1, num_epochs + 1)\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(epochs_range, history['train_loss'], 'o-', label='Training Loss')\n",
        "    plt.plot(epochs_range, history['test_loss'], 'o-', label='Test Loss')\n",
        "    plt.title(f'Training and Test Loss (LR={learning_rate})')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(epochs_range, history['train_acc'], 'o-', label='Training Accuracy')\n",
        "    plt.plot(epochs_range, history['test_acc'], 'o-', label='Test Accuracy')\n",
        "    plt.title(f'Training and Test Accuracy (LR={learning_rate})')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot learning rate\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(epochs_range, history['lr'], 'o-')\n",
        "    plt.title('Learning Rate Schedule')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'finetuning_lr_{learning_rate}.png')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Final Test Accuracy: {best_acc:.2f}%\")\n",
        "\n",
        "    return model, history, best_acc\n",
        "\n",
        "def train_last_layer(model, train_loader, test_loader, learning_rate, num_epochs=60, momentum=0.9, weight_decay=5e-4):\n",
        "    \"\"\"\n",
        "    Train only the last layer of the model (feature extraction)\n",
        "    \"\"\"\n",
        "    # Freeze all layers except the last one\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Unfreeze the last layer\n",
        "    for param in model.fc.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # Define criterion and optimizer (only train the last layer)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.fc.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "    # Define learning rate scheduler with multi-step decay\n",
        "    milestones = [int(num_epochs * 0.25), int(num_epochs * 0.5), int(num_epochs * 0.75)]\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n",
        "\n",
        "    # Initialize best accuracy and model\n",
        "    best_acc = 0.0\n",
        "    best_model = None\n",
        "\n",
        "    # Training and validation history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'test_loss': [],\n",
        "        'test_acc': [],\n",
        "        'lr': []\n",
        "    }\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()"
      ],
      "metadata": {
        "id": "MrpYFZx4SxQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4 - Weakly and Semi-Supervised Learning for Image Classification"
      ],
      "metadata": {
        "id": "a1CyIXN1Uatc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Parameters K and P in Stage 2 (Yalniz et al.)\n",
        "\n",
        "K is the number of top‐ranked images that the teacher model selects per class from the unlabeled pool U. After running the teacher on every image, images are ranked by the teacher’s softmax score for each class, and the top K images for class l form the pseudo‐labeled set D̂ₗ for that class.\n",
        "\n",
        "P is the number of highest‐scoring classes (per image) that the teacher retains when assigning candidates. For each unlabeled image, we take its P largest softmax outputs (rather than just the single top class), on the intuition that an image may legitimately contain multiple target classes—especially under‐represented or co‐occurring concepts that might be occluded by more prominent ones\n",
        "arXiv\n",
        "arXiv\n",
        ".\n",
        "\n",
        "Why P > 1? By allowing each image to contribute to up to P classes, the procedure avoids starving rare classes of examples when the overall collection U isn’t large enough. It also yields a more balanced D̂ by letting an image appear in multiple class‐specific rankings\n",
        "arXiv\n",
        ".\n",
        "\n",
        "(c) Creating a New Labeled Dataset (Yalniz et al.)\n",
        "\n",
        "Teacher predictions: Run the teacher model (trained on the small labeled set D) on each image in the unlabeled set U to get a softmax score vector.\n",
        "\n",
        "Class shortlist per image: For each image, keep only the P classes with highest scores.\n",
        "\n",
        "Per-class ranking: For each class l, rank all images by their score for l, and select the top K as positive examples for that class—this yields D̂ₗ.\n",
        "\n",
        "Aggregate: Form the new dataset D̂ = ⋃ₗ D̂ₗ.\n",
        "\n",
        "Replication for multi-class: If an image is in multiple D̂ₗ sets, it’s simply replicated (once per class) in D̂; the student is then trained as a standard multi‐class classifier on D̂\n",
        "arXiv\n",
        ".\n",
        "\n",
        "Can an image belong to more than one class?\n",
        "Yes. Because we allow P > 1 and then rank per class, an image can appear in multiple class‐specific top‐K lists. We handle this by replicating the image under each of its assigned classes, but still train the student with a softmax over the union D̂.\n",
        "\n",
        "(d) Why Student Accuracy First Improves then Drops as K Increases (Figure 5)\n",
        "\n",
        "Initial gains: As K grows from small values, the student sees more diverse and harder examples per class, which boosts its ability to generalize.\n",
        "\n",
        "Plateau: Across a broad regime (e.g. 4 k–32 k), accuracy is stable—small adjustments to K matter less.\n",
        "\n",
        "Decline: Beyond the sweet spot, including lower‐ranked images admits many false positives (label noise), which degrades student performance.\n",
        "\n",
        "This trade‐off peaks around K = 8 k for ResNet-50 and a bit higher for larger students\n",
        "arXiv\n",
        ".\n",
        "\n",
        "1. Weakly Supervised vs Semi-Supervised Pretraining\n",
        "\n",
        "Weakly supervised pretraining (Mahajan et al.) uses the hashtags themselves as noisy labels. A model is trained directly on billions of hashtagged images W to predict those hashtags, then fine-tuned on a smaller, clean set V. This leverages large weakly annotated data end-to-end\n",
        "arXiv\n",
        ".\n",
        "\n",
        "Semi-supervised pretraining (Yalniz et al.) starts from a small set of truly labeled images D to train a teacher. That teacher then pseudo-labels a large unlabeled pool U (ignoring hashtags), producing a new noisy but expansive D̂ on which a student is trained and finally fine-tuned back on D\n",
        "arXiv\n",
        ".\n",
        "\n",
        "Same dataset, two uses: Both papers mine the same 1 B–image pool (IG-1B‐Targeted). Mahajan et al. treat each image’s hashtag as its “ground truth” (weak labels), whereas Yalniz et al. discard those hashtags and treat the images as unlabeled, relying instead on a teacher’s predictions for pseudo‐labeling\n",
        "arXiv\n",
        ".\n",
        "\n",
        "2. From Mahajan et al.\n",
        "(a) Robustness to Label Noise\n",
        "\n",
        "They injected p % artificial label noise by randomly replacing p % of hashtags with random tags sampled from the overall hashtag distribution.\n",
        "\n",
        "Findings: At p = 10 %, ImageNet top-1 accuracy drops by < 1 %; at p = 25 %, the drop is ≈ 2 %. This shows surprising resilience to noisy hashtag supervision when training on billions of examples\n",
        "arXiv\n",
        ".\n",
        "\n",
        "(b) Importance of Resampling the Hashtag Distribution\n",
        "\n",
        "Instagram hashtags follow a Zipfian (long‐tailed) distribution. If you sample “naturally,” head hashtags dominate training.\n",
        "\n",
        "By using square-root or uniform resampling (i.e., under-sampling head tags, over-sampling tail tags), they observe 5–6 % absolute gains in transfer accuracy across target tasks, versus natural sampling. This balanced sampling is crucial to expose the model to rare concepts and improve feature quality\n",
        "arXiv\n",
        ".\n",
        "\n",
        "3. Teacher–Student Distillation (Yalniz et al.)\n",
        "\n",
        "Two Models:\n",
        "\n",
        "Teacher: trained on the small, clean labeled set D.\n",
        "\n",
        "Student: trained on the large pseudo-labeled set D̂ produced by the teacher.\n",
        "\n",
        "Leverage: The student “distills” knowledge by mimicking the teacher’s predictions on unlabeled data. This extends classic model compression: instead of distilling soft logits on the same data, here we distill across a vast unlabeled corpus, effectively amplifying the teacher’s knowledge.\n",
        "\n",
        "Why Distillation: In both cases, the student is optimized to reproduce the teacher’s outputs—either soft targets (logits) or hard pseudo-labels—on extra data. This is precisely the core of knowledge distillation, where a large (or strongly supervised) teacher guides a smaller (or target) student to reach higher accuracy than direct supervised training alone\n",
        "arXiv\n",
        ".\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x3snt5v1Ue8I"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "71gakbqeUcde"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}